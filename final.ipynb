{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, codecs,os,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.wrappers import LdaMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './corpus_1900_1910/4/' #filename (.txt)\n",
    "stopfilename = 'stop.txt'  # file with stop-words\n",
    "filename = 'out.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(PATH)[:15]\n",
    "texts = []\n",
    "for name in file_names:\n",
    "    if name.endswith(\".txt\"):\n",
    "        with codecs.open(PATH + \"/\" + name, encoding = 'utf-8') as f:\n",
    "            #print(name)\n",
    "            text = f.read()\n",
    "            lst = re.findall(r'\\w+', text)\n",
    "            words = []\n",
    "            for word in lst:\n",
    "                lemma = morph.parse(word)[0]  # parsing\n",
    "                words.append(lemma.normal_form) #add a lemma to words\n",
    "            \n",
    "            texts.append(words)\n",
    "            \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "for i in dictionary.keys():\n",
    "    print (dictionary[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Clining the dictionary  #### \n",
    "# deleting of stop-words \n",
    "\n",
    "stopfile = open('stop.txt', 'r') # file with stop-words with coding utf8\n",
    "stopfile = codecs.open(stopfilename, 'r', encoding = 'utf8')\n",
    "stopwords = stopfile.read()\n",
    "stoplist = set(stopwords.split())\n",
    "stopfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, codecs\n",
    "\n",
    "# logging.basicConfig(format='%\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "             if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids) # удаляем слова\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "once_ids = [tokenid for tokenid,docfreq in dictionary.dfs.items() if docfreq < 5] \n",
    "dictionary.filter_tokens(once_ids)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.compactify() # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dictionary, for future reference\n",
    "dictionary.save_as_text(filename + '_dict.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LDA ###############\n",
    "corpus = []\n",
    "for document in file_names:\n",
    "    if name.endswith(\".txt\"):\n",
    "        with codecs.open(PATH + \"/\" + document, encoding = 'utf-8')as m:\n",
    "            corpus.append(dictionary.doc2bow(m.read().lower().split()))\n",
    "corpora.MmCorpus.serialize(filename + '_corpus.mm', corpus)\n",
    "      \n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load id->word mapping (the dictionary)\n",
    "id2word = corpora.Dictionary.load_from_text(filename + '_dict.txt')\n",
    "mm = corpora.MmCorpus(filename + '_corpus.mm') # Matrix Market format.\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, \n",
    "                               id2word=id2word, \n",
    "                               num_topics=10, # quantity of topics from the corpus \n",
    "                               update_every=1,#quantity of documents to be iterated through for each update. Set to 0 for batch learning, > 1 for online iterative learning\n",
    "                               chunksize=100, #quantity of documents for using in every chunk\n",
    "                               passes=10 # quantity of iterations through the corpus .\n",
    "                             # alpha='auto',\n",
    "                             # per_word_topics=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print words from (number) random topics \n",
    "topics = lda.print_topics(10)\n",
    "for topic in topics:\n",
    "    print (topic[0],':',topic[1],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda.print_topics()) ### perplexity for (number) topics ###\n",
    "doc_lda = lda[corpus]\n",
    "print('\\nPerplexity: ', lda.log_perplexity(corpus))  # the result shows how good the model is. The lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_v Measure\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMass\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence=\"u_mass\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook() \n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = lda\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "#lda = models.ldamodel.LdaModel(corpus=mm, \n",
    " #                              id2word=id2word, # Mapping from word IDs to words. to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "  #                             num_topics=10, #The number of requested latent topics to be extracted from the training corpus.\n",
    "   #                            update_every=1,#Number of documents to be iterated through for each update. Set to 0 for batch learning, > 1 for online iterative learning\n",
    "    #                           chunksize=100, #Number of documents to be used in each training chunk\n",
    "     #                          passes=10 #Number of passes through the corpus during training.\n",
    "                             # alpha='auto',\n",
    "                             # per_word_topics=True\n",
    "        #                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=2, limit=40, step=4)\n",
    "\n",
    "limit=40; start=2; step=4;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[5]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most representative document for each topic\n",
    "\n",
    "def format_topics_sentences(ldamodel=lda, corpus=corpus, texts=texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
